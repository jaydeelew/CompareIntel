"""
Backward-compatible shim. LLM logic lives in app.llm package.
"""

from app.llm import (
    FREE_TIER_MODELS,
    MODELS_BY_PROVIDER,
    OPENROUTER_MODELS,
    UNREGISTERED_TIER_MODELS,
    calculate_credits,
    calculate_token_usage,
    call_openrouter,
    call_openrouter_streaming,
    clean_model_response,
    client,
    client_with_tool_headers,
    count_conversation_tokens,
    detect_repetition,
    estimate_credits_before_request,
    estimate_token_count,
    fetch_all_models_from_openrouter,
    fetch_url_content,
    filter_models_by_tier,
    get_min_max_input_tokens,
    get_min_max_output_tokens,
    get_model_max_input_tokens,
    get_model_max_output_tokens,
    get_model_max_tokens,
    get_model_token_limits_from_openrouter,
    is_model_available_for_tier,
    is_time_sensitive_query,
    preload_model_token_limits,
    refresh_model_token_limits,
    sort_models_by_tier_and_version,
    test_connection_quality,
)
from app.llm.tokens import TokenUsage

__all__ = [
    "FREE_TIER_MODELS",
    "MODELS_BY_PROVIDER",
    "OPENROUTER_MODELS",
    "TokenUsage",
    "UNREGISTERED_TIER_MODELS",
    "call_openrouter",
    "call_openrouter_streaming",
    "calculate_credits",
    "calculate_token_usage",
    "clean_model_response",
    "client",
    "client_with_tool_headers",
    "count_conversation_tokens",
    "detect_repetition",
    "estimate_credits_before_request",
    "estimate_token_count",
    "fetch_all_models_from_openrouter",
    "fetch_url_content",
    "filter_models_by_tier",
    "get_min_max_input_tokens",
    "get_min_max_output_tokens",
    "get_model_max_input_tokens",
    "get_model_max_output_tokens",
    "get_model_max_tokens",
    "get_model_token_limits_from_openrouter",
    "is_model_available_for_tier",
    "is_time_sensitive_query",
    "preload_model_token_limits",
    "refresh_model_token_limits",
    "sort_models_by_tier_and_version",
    "test_connection_quality",
]
